INFO: STARTING!
INFO: Log parameter values verbosely: [0]
INFO: Min num runs: [100]
INFO: Num threads: [4]
INFO: Min warmup runs: [10]
INFO: Graph: [/data/local/tmp/mobilenet_v2_fp32.tflite]
INFO: Signature to run: []
INFO: #threads used for CPU inference: [4]
INFO: Use gpu: [0]
INFO: Use NNAPI: [0]
INFO: Loaded model /data/local/tmp/mobilenet_v2_fp32.tflite
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
VERBOSE: Replacing 65 out of 65 node(s) with delegate (TfLiteXNNPackDelegate) node, yielding 1 partitions for subgraph 0.
INFO: The input model file size (MB): 13.9849
INFO: Initialized session in 39.58ms.
INFO: Running benchmark for at least 10 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
INFO: count=23 first=29210 curr=21053 min=20903 max=29210 avg=21756.6 std=1666 p5=20989 median=21215 p95=23009

INFO: Running benchmark for at least 100 iterations and at least 1 seconds but terminate if exceeding 150 seconds.
INFO: count=100 first=21091 curr=20917 min=20574 max=24026 avg=21479.2 std=548 p5=20744 median=21430 p95=22318

INFO: Inference timings in us: Init: 39580, First inference: 29210, Warmup (avg): 21756.6, Inference (avg): 21479.2
INFO: Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
INFO: Memory footprint delta from the start of the tool (MB): init=38 overall=39.125
