INFO: STARTING!
INFO: Log parameter values verbosely: [0]
INFO: Min num runs: [100]
INFO: Num threads: [4]
INFO: Min warmup runs: [10]
INFO: Graph: [/data/local/tmp/mobilenet_v2_int8.tflite]
INFO: Signature to run: []
INFO: #threads used for CPU inference: [4]
INFO: Use gpu: [0]
INFO: Use NNAPI: [0]
INFO: Loaded model /data/local/tmp/mobilenet_v2_int8.tflite
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
VERBOSE: Replacing 64 out of 67 node(s) with delegate (TfLiteXNNPackDelegate) node, yielding 3 partitions for subgraph 0.
INFO: The input model file size (MB): 4.01403
INFO: Initialized session in 31.373ms.
INFO: Running benchmark for at least 10 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
INFO: count=55 first=11902 curr=8979 min=8312 max=12868 avg=9113.58 std=1042 p5=8335 median=8673 p95=11902

INFO: Running benchmark for at least 100 iterations and at least 1 seconds but terminate if exceeding 150 seconds.
INFO: count=112 first=9424 curr=8864 min=8289 max=12192 avg=8867.67 std=648 p5=8354 median=8668 p95=10401

INFO: Inference timings in us: Init: 31373, First inference: 11902, Warmup (avg): 9113.58, Inference (avg): 8867.67
INFO: Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
INFO: Memory footprint delta from the start of the tool (MB): init=14.25 overall=14.75
